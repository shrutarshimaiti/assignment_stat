{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1.Explain the properties of the F-distribution.\n",
        "A-Properties of the F-distribution\n",
        "The F-distribution is a continuous probability distribution that arises frequently in statistical tests, particularly those comparing variances or analyzing variance (ANOVA). Here are its key properties:\n",
        "\n",
        "Shape and Characteristics:\n",
        "\n",
        "The F-distribution is positively skewed and is defined only for non-negative values.\n",
        "The shape depends on two parameters: the degrees of freedom for the numerator (\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "df\n",
        "1\n",
        "​\n",
        " ) and the denominator (\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "df\n",
        "2\n",
        "​\n",
        " ).\n",
        "Degrees of Freedom:\n",
        "\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "df\n",
        "1\n",
        "​\n",
        " : Associated with the variance estimate in the numerator.\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "df\n",
        "2\n",
        "​\n",
        " : Associated with the variance estimate in the denominator.\n",
        "Larger degrees of freedom make the distribution approach a normal distribution.\n",
        "Range:\n",
        "\n",
        "The F-distribution takes values in the interval\n",
        "[\n",
        "0\n",
        ",\n",
        "∞\n",
        ")\n",
        "[0,∞).\n",
        "Mean:\n",
        "\n",
        "For\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        ">\n",
        "2\n",
        "df\n",
        "2\n",
        "​\n",
        " >2, the mean of the F-distribution is approximately:\n",
        "Mean\n",
        "=\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "−\n",
        "2\n",
        ".\n",
        "Mean=\n",
        "df\n",
        "2\n",
        "​\n",
        " −2\n",
        "df\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        " .\n",
        "Variance:\n",
        "\n",
        "For\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        ">\n",
        "4\n",
        "df\n",
        "2\n",
        "​\n",
        " >4, the variance is:\n",
        "Variance\n",
        "=\n",
        "2\n",
        "⋅\n",
        "(\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        ")\n",
        "2\n",
        "⋅\n",
        "(\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "+\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "−\n",
        "2\n",
        ")\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "⋅\n",
        "(\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "−\n",
        "2\n",
        ")\n",
        "2\n",
        "⋅\n",
        "(\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "−\n",
        "4\n",
        ")\n",
        ".\n",
        "Variance=\n",
        "df\n",
        "1\n",
        "​\n",
        " ⋅(df\n",
        "2\n",
        "​\n",
        " −2)\n",
        "2\n",
        " ⋅(df\n",
        "2\n",
        "​\n",
        " −4)\n",
        "2⋅(df\n",
        "2\n",
        "​\n",
        " )\n",
        "2\n",
        " ⋅(df\n",
        "1\n",
        "​\n",
        " +df\n",
        "2\n",
        "​\n",
        " −2)\n",
        "​\n",
        " .\n",
        "Applications:\n",
        "\n",
        "Commonly used in the analysis of variance (ANOVA) and regression analysis.\n",
        "Used in F-tests to compare two population variances.\n",
        "Non-Negativity:\n",
        "\n",
        "Since the F-distribution is derived from ratios of squared variances, it cannot take negative values.\n",
        "Asymmetry:\n",
        "\n",
        "The skewness decreases as the degrees of freedom increase, and the distribution becomes more symmetric."
      ],
      "metadata": {
        "id": "k0LZd_3D1TC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "A-Statistical Tests That Use the F-distribution and Their Appropriateness\n",
        "The F-distribution is used in several statistical tests because it is well-suited for analyzing ratios of variances. Here are the primary types of tests and their rationale:\n",
        "\n",
        "1. F-test for Equality of Variances:\n",
        "Purpose: To compare the variances of two populations.\n",
        "Appropriateness:\n",
        "The F-statistic is calculated as the ratio of the sample variances (\n",
        "𝐹\n",
        "=\n",
        "𝑠\n",
        "1\n",
        "2\n",
        "𝑠\n",
        "2\n",
        "2\n",
        "F=\n",
        "s\n",
        "2\n",
        "2\n",
        "​\n",
        "\n",
        "s\n",
        "1\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        " ).\n",
        "The F-distribution is appropriate because it models the behavior of this ratio under the null hypothesis (equal variances).\n",
        "2. Analysis of Variance (ANOVA):\n",
        "Purpose: To test whether the means of three or more groups are significantly different.\n",
        "Appropriateness:\n",
        "ANOVA partitions total variance into between-group and within-group variance and uses their ratio to compute the F-statistic.\n",
        "The F-distribution is ideal because it quantifies how much of the variance is explained by group differences versus random error.\n",
        "3. Regression Analysis:\n",
        "Purpose: To assess the significance of predictors in a regression model.\n",
        "Appropriateness:\n",
        "The F-test is used to evaluate the overall fit of the model by comparing the regression sum of squares to the error sum of squares.\n",
        "The F-distribution is used to test whether the explained variance (due to predictors) is significant compared to the unexplained variance (error).\n",
        "4. Comparison of Nested Models:\n",
        "Purpose: To compare a complex model with a simpler (nested) model.\n",
        "Appropriateness:\n",
        "The F-test assesses whether adding more predictors significantly improves the model.\n",
        "The F-distribution is used because it accounts for the degrees of freedom associated with each model.\n",
        "Why the F-distribution is Appropriate:\n",
        "Ratio of Variances: The F-distribution naturally arises in situations involving ratios of independent chi-squared distributions (e.g., variances).\n",
        "Non-negativity: Variances are non-negative, and the F-distribution is defined only for non-negative values.\n",
        "Degrees of Freedom: The shape of the F-distribution adjusts based on the sample sizes, reflecting the precision of variance estimates."
      ],
      "metadata": {
        "id": "Hx9ap3D31bu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?\n",
        "A-Key Assumptions for Conducting an F-test to Compare Variances\n",
        "When performing an F-test to compare the variances of two populations, certain assumptions must be met to ensure the validity of the test. These assumptions are as follows:\n",
        "\n",
        "1. Independence of Observations:\n",
        "The data within each sample must be independent.\n",
        "This means that the value of one observation does not influence or relate to another observation within the same sample or between samples.\n",
        "2. Normality:\n",
        "The populations from which the samples are drawn must follow a normal distribution.\n",
        "While the F-test is somewhat robust to minor deviations from normality, significant departures can lead to inaccurate results.\n",
        "3. Random Sampling:\n",
        "Both samples must be drawn randomly from their respective populations.\n",
        "Random sampling ensures that the data is representative and minimizes biases.\n",
        "4. Positive Variances:\n",
        "The variances of the populations being compared must be positive.\n",
        "Since variance is a squared value, negative variances are not possible.\n",
        "5. Equal Sample Sizes (Optional):\n",
        "While not strictly required, equal sample sizes in both groups improve the reliability of the test.\n",
        "Unequal sample sizes can affect the power of the test, especially if the variances are highly different.\n",
        "Practical Considerations:\n",
        "Robust Alternatives: If these assumptions are violated, consider using non-parametric tests like Levene's test or the Brown-Forsythe test, which are more robust to deviations from normality.\n",
        "Transformation: For highly skewed data, applying a logarithmic or other transformation might help achieve normality."
      ],
      "metadata": {
        "id": "HoaCy7pn1xVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.4 What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "A-\n",
        "Purpose of ANOVA and Its Differences from a t-test\n",
        "Purpose of ANOVA:\n",
        "The primary purpose of ANOVA (Analysis of Variance) is to determine whether there are statistically significant differences between the means of three or more groups. It achieves this by comparing the variance within groups to the variance between groups.\n",
        "\n",
        "Key aspects of ANOVA:\n",
        "\n",
        "Hypotheses:\n",
        "Null hypothesis (\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " ): All group means are equal.\n",
        "Alternative hypothesis (\n",
        "𝐻\n",
        "𝑎\n",
        "H\n",
        "a\n",
        "​\n",
        " ): At least one group mean is different.\n",
        "F-statistic: The ratio of the variance between groups to the variance within groups.\n",
        "Applications: Frequently used in experiments with multiple groups, such as testing the effects of different treatments or conditions.\n",
        "Differences Between ANOVA and a t-test:\n",
        "Aspect\tt-test\tANOVA\n",
        "Number of Groups\tCompares the means of two groups only.\tCompares the means of three or more groups.\n",
        "Hypothesis\tTests if the means of two groups are equal.\tTests if at least one mean differs among groups.\n",
        "Type of Variance\tUses a single estimate of variance.\tPartitions variance into between-group and within-group components.\n",
        "Test Statistic\tProduces a t-statistic.\tProduces an F-statistic.\n",
        "Error Risk\tComparing multiple pairs with t-tests increases Type I error.\tANOVA controls for Type I error in multiple group comparisons.\n",
        "Post-hoc Analysis\tNot applicable; results are straightforward.\tMay require further analysis (e.g., Tukey's test) to identify specific group differences.\n",
        "Why Use ANOVA Instead of Multiple t-tests?\n",
        "When comparing more than two groups, using multiple t-tests increases the risk of committing a Type I error (false positive). ANOVA addresses this by testing all groups simultaneously under a unified framework."
      ],
      "metadata": {
        "id": "jM7wRGBv2Mjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oDHY_Ubg3AKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.\n",
        "A-A one-way ANOVA is appropriate when:\n",
        "\n",
        "You want to compare the means of three or more independent groups.\n",
        "There is one independent variable (or factor) that categorizes the groups, and one dependent variable to measure.\n",
        "For example:\n",
        "\n",
        "Testing whether three different diets have distinct effects on weight loss.\n",
        "Comparing the average scores of students in three different classes.\n",
        "Why Use a One-Way ANOVA Instead of Multiple t-Tests?\n",
        "Controls Type I Error:\n",
        "\n",
        "Each t-test comes with a risk of a Type I error (rejecting a true null hypothesis).\n",
        "When conducting multiple t-tests, the cumulative Type I error increases. For example, with three groups (\n",
        "𝐴\n",
        ",\n",
        "𝐵\n",
        ",\n",
        "𝐶\n",
        "A,B,C), you would perform three t-tests (\n",
        "𝐴\n",
        "A vs\n",
        "𝐵\n",
        "B,\n",
        "𝐴\n",
        "A vs\n",
        "𝐶\n",
        "C,\n",
        "𝐵\n",
        "B vs\n",
        "𝐶\n",
        "C), leading to a higher overall error rate.\n",
        "ANOVA evaluates all groups simultaneously, maintaining the overall Type I error at the chosen significance level (e.g.,\n",
        "𝛼\n",
        "=\n",
        "0.05\n",
        "α=0.05).\n",
        "Efficiency:\n",
        "\n",
        "One-way ANOVA is computationally simpler and faster than performing multiple pairwise t-tests, especially with many groups.\n",
        "Comprehensive Comparison:\n",
        "\n",
        "ANOVA identifies whether there is any difference among group means. In contrast, t-tests only compare specific pairs of groups.\n",
        "Post-hoc tests (e.g., Tukey's test) can then identify which groups differ if ANOVA finds a significant result.\n",
        "Interpreting Variance:\n",
        "\n",
        "ANOVA partitions the total variance into between-group and within-group variances, providing a clearer understanding of group differences and variation.\n",
        "Illustrative Example:\n",
        "Suppose you are comparing the test scores of students from three regions:\n",
        "\n",
        "Region A:\n",
        "[\n",
        "85\n",
        ",\n",
        "87\n",
        ",\n",
        "90\n",
        ",\n",
        "88\n",
        ",\n",
        "86\n",
        "]\n",
        "[85,87,90,88,86]\n",
        "Region B:\n",
        "[\n",
        "78\n",
        ",\n",
        "80\n",
        ",\n",
        "83\n",
        ",\n",
        "79\n",
        ",\n",
        "81\n",
        "]\n",
        "[78,80,83,79,81]\n",
        "Region C:\n",
        "[\n",
        "92\n",
        ",\n",
        "94\n",
        ",\n",
        "91\n",
        ",\n",
        "93\n",
        ",\n",
        "95\n",
        "]\n",
        "[92,94,91,93,95]\n",
        "If you perform t-tests:\n",
        "\n",
        "Compare Region A vs Region B\n",
        "Compare Region A vs Region C\n",
        "Compare Region B vs Region C\n",
        "This would lead to three separate tests, each with a risk of Type I error. With a one-way ANOVA, you perform a single test to check if there is any significant difference across all three groups.\n",
        "\n"
      ],
      "metadata": {
        "id": "fNNGgwbB3ATN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6.Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?\n",
        "A-\n",
        "Partitioning Variance in ANOVA: Between-Group and Within-Group Variance\n",
        "ANOVA (Analysis of Variance) analyzes the total variability in the data by partitioning it into two components:\n",
        "\n",
        "Between-group variance: Variability due to differences between the group means.\n",
        "Within-group variance: Variability due to differences within each group.\n",
        "1. Total Variance:\n",
        "The total variability in the dataset is represented by the total sum of squares (SST):\n",
        "\n",
        "SST\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "SST=\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " (X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )\n",
        "2\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        " : Individual data points.\n",
        "𝑋\n",
        "ˉ\n",
        "X\n",
        "ˉ\n",
        " : Grand mean (mean of all data points combined).\n",
        "2. Between-Group Variance:\n",
        "This measures how much the group means differ from the grand mean. The sum of squares between groups (SSB) is:\n",
        "\n",
        "SSB\n",
        "=\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑛\n",
        "𝑗\n",
        "(\n",
        "𝑋\n",
        "ˉ\n",
        "𝑗\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "SSB=\n",
        "j=1\n",
        "∑\n",
        "k\n",
        "​\n",
        " n\n",
        "j\n",
        "​\n",
        " (\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "j\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )\n",
        "2\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑋\n",
        "ˉ\n",
        "𝑗\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "j\n",
        "​\n",
        " : Mean of group\n",
        "𝑗\n",
        "j.\n",
        "𝑛\n",
        "𝑗\n",
        "n\n",
        "j\n",
        "​\n",
        " : Number of observations in group\n",
        "𝑗\n",
        "j.\n",
        "𝑘\n",
        "k: Number of groups.\n",
        "3. Within-Group Variance:\n",
        "This measures the variability of individual observations within each group. The sum of squares within groups (SSW) is:\n",
        "\n",
        "SSW\n",
        "=\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑗\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "𝑗\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        "𝑗\n",
        ")\n",
        "2\n",
        "SSW=\n",
        "j=1\n",
        "∑\n",
        "k\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "n\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        " (X\n",
        "ij\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "j\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑋\n",
        "𝑖\n",
        "𝑗\n",
        "X\n",
        "ij\n",
        "​\n",
        " : Individual data point in group\n",
        "𝑗\n",
        "j.\n",
        "𝑋\n",
        "ˉ\n",
        "𝑗\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "j\n",
        "​\n",
        " : Mean of group\n",
        "𝑗\n",
        "j.\n",
        "Relationship Between Variance Components:\n",
        "SST\n",
        "=\n",
        "SSB\n",
        "+\n",
        "SSW\n",
        "SST=SSB+SSW\n",
        "This equation shows how the total variability is divided into variability due to group differences (\n",
        "𝑆\n",
        "𝑆\n",
        "𝐵\n",
        "SSB) and variability within the groups (\n",
        "𝑆\n",
        "𝑆\n",
        "𝑊\n",
        "SSW).\n",
        "\n",
        "Calculating the F-statistic:\n",
        "The F-statistic is the ratio of the between-group variance to the within-group variance:\n",
        "\n",
        "𝐹\n",
        "=\n",
        "MSB\n",
        "MSW\n",
        "F=\n",
        "MSW\n",
        "MSB\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "MSB\n",
        "=\n",
        "SSB\n",
        "𝑘\n",
        "−\n",
        "1\n",
        "MSB=\n",
        "k−1\n",
        "SSB\n",
        "​\n",
        " : Mean square between groups.\n",
        "MSW\n",
        "=\n",
        "SSW\n",
        "𝑛\n",
        "−\n",
        "𝑘\n",
        "MSW=\n",
        "n−k\n",
        "SSW\n",
        "​\n",
        " : Mean square within groups.\n",
        "𝑘\n",
        "k: Number of groups.\n",
        "𝑛\n",
        "n: Total number of observations.\n",
        "How Variance Partitioning Contributes to the F-statistic:\n",
        "If the group means are similar,\n",
        "SSB\n",
        "SSB will be small, resulting in a smaller F-statistic, indicating no significant difference between groups.\n",
        "If the group means are different,\n",
        "SSB\n",
        "SSB will be large, resulting in a larger F-statistic, which may indicate significant group differences."
      ],
      "metadata": {
        "id": "tWhR-_c43i3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "A-\n",
        "Comparing the Classical (Frequentist) Approach to ANOVA with the Bayesian Approach\n",
        "Both the classical (frequentist) and Bayesian approaches to ANOVA aim to analyze differences between group means, but they differ significantly in their underlying principles, assumptions, and interpretation. Here's a detailed comparison:\n",
        "\n",
        "1. Underlying Philosophy:\n",
        "Classical ANOVA:\n",
        "\n",
        "Based on frequentist statistics, which relies on long-run frequencies of outcomes.\n",
        "Uses p-values to determine the probability of observing the data if the null hypothesis (\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " ) is true.\n",
        "Results are interpreted in the context of rejecting or failing to reject\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " .\n",
        "Bayesian ANOVA:\n",
        "\n",
        "Based on Bayesian statistics, which updates prior beliefs using observed data to obtain posterior probabilities.\n",
        "Provides probabilities for hypotheses being true directly, rather than relying on p-values.\n",
        "2. Treatment of Uncertainty:\n",
        "Classical ANOVA:\n",
        "\n",
        "Assumes fixed parameters (e.g., group means) and calculates uncertainty only from the sampling process.\n",
        "Does not provide direct probabilities for hypotheses.\n",
        "Bayesian ANOVA:\n",
        "\n",
        "Treats parameters as random variables and incorporates uncertainty in prior distributions.\n",
        "Quantifies uncertainty in terms of posterior probabilities, credible intervals, and probability distributions.\n",
        "3. Hypothesis Testing:\n",
        "Classical ANOVA:\n",
        "\n",
        "Null hypothesis: All group means are equal (\n",
        "𝐻\n",
        "0\n",
        ":\n",
        "𝜇\n",
        "1\n",
        "=\n",
        "𝜇\n",
        "2\n",
        "=\n",
        "⋯\n",
        "=\n",
        "𝜇\n",
        "𝑘\n",
        "H\n",
        "0\n",
        "​\n",
        " :μ\n",
        "1\n",
        "​\n",
        " =μ\n",
        "2\n",
        "​\n",
        " =⋯=μ\n",
        "k\n",
        "​\n",
        " ).\n",
        "Tests against the alternative hypothesis that at least one group mean differs.\n",
        "Uses an F-statistic and p-value to determine statistical significance.\n",
        "Bayesian ANOVA:\n",
        "\n",
        "Directly compares the posterior probabilities of models (e.g.,\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        "  vs\n",
        "𝐻\n",
        "𝑎\n",
        "H\n",
        "a\n",
        "​\n",
        " ).\n",
        "Does not rely on p-values but instead provides a Bayes Factor (ratio of likelihoods of competing models).\n",
        "4. Parameter Estimation:\n",
        "Classical ANOVA:\n",
        "\n",
        "Estimates parameters (e.g., group means) using sample data without incorporating prior knowledge.\n",
        "Bayesian ANOVA:\n",
        "\n",
        "Combines prior knowledge (if available) with observed data to estimate parameters.\n",
        "Posterior distributions provide richer information than point estimates.\n",
        "5. Interpretability:\n",
        "Classical ANOVA:\n",
        "\n",
        "Results are binary (reject or fail to reject\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " ).\n",
        "P-values and F-statistics can be unintuitive for non-statisticians.\n",
        "Bayesian ANOVA:\n",
        "\n",
        "Results are intuitive, providing probabilities for hypotheses and credible intervals for parameters.\n",
        "6. Flexibility:\n",
        "Classical ANOVA:\n",
        "\n",
        "Limited to certain types of designs and assumptions (e.g., normality, equal variances).\n",
        "Bayesian ANOVA:\n",
        "\n",
        "More flexible in handling complex models and situations where assumptions (e.g., normality) are violated.\n",
        "Example:\n",
        "In a classical ANOVA, a p-value of 0.03 might lead to rejecting\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " , concluding a significant difference between group means.\n",
        "In a Bayesian ANOVA, you might find that the probability of\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        "  being true is 5%, while the probability of\n",
        "𝐻\n",
        "𝑎\n",
        "H\n",
        "a\n",
        "​\n",
        "  (group means differ) is 95%, giving a more direct interpretation.\n"
      ],
      "metadata": {
        "id": "4THb_99H4RXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8."
      ],
      "metadata": {
        "id": "HoHgyOyb5Rjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Data\n",
        "profession_a = [48, 52, 55, 60, 62]\n",
        "profession_b = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate variances\n",
        "var_a = np.var(profession_a, ddof=1)  # Sample variance for Profession A\n",
        "var_b = np.var(profession_b, ddof=1)  # Sample variance for Profession B\n",
        "\n",
        "# Calculate the F-statistic\n",
        "F_statistic = var_a / var_b if var_a > var_b else var_b / var_a\n",
        "\n",
        "# Degrees of freedom\n",
        "df1 = len(profession_a) - 1\n",
        "df2 = len(profession_b) - 1\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = 2 * min(f.cdf(F_statistic, df1, df2), 1 - f.cdf(F_statistic, df1, df2))\n",
        "\n",
        "# Output results\n",
        "print(f\"F-statistic: {F_statistic}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n",
        "# Conclusion\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The variances are not significantly different.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI3kdQSm5-Uv",
        "outputId": "f2da0262-8366-4ae6-ccbe-0daba3dd86df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.49304859900533904\n",
            "Fail to reject the null hypothesis: The variances are not significantly different.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9."
      ],
      "metadata": {
        "id": "Gp6Ugcxv6yJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Data for each region\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA test\n",
        "f_statistic, p_value = f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Output the results\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n",
        "# Hypothesis conclusion based on the p-value\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference in mean heights between the regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The mean heights between the regions are not significantly different.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoAW94-56zOe",
        "outputId": "2655db03-5f04-4a49-d19d-61faa6cf11c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n",
            "Reject the null hypothesis: There is a significant difference in mean heights between the regions.\n"
          ]
        }
      ]
    }
  ]
}